---
title: PEML Data Model for Software Tests
---
<section>
  <h2 id="tests">Specifying Tests</h2>
  <p><span class="badge badge-warning">Still writing this! ...</span></p>

  <p>We expect there to be a wide variety of test specification formats,
    and they may not all be supported by all automated grading tools. Many
    may be programming-language-specific, and only suitable for exercises
    where solutions are written in that one language. However, we feel that
    supporting some variety here is much better than forcing everyone to
    conform to a single format, and limiting assessment actions (and feedback
    possibilities) to that one style only.</p>
    
  <p>Although we're still missing a lot of the details here, we expect that
    this section will include (but not be limited to) at least the
    following:</p>
  <ul>
    <li><p><b>Stdin/stdout text segments</b>, for testing main programs that
      read/write text. This will probably include ClodeCoder-style regular
      expression support plus some Web-CAT-inspired normalization options
      so that less-brittle matching can be supported with little effort.
    </p></li>
    <li><p><b>Tables of input values and expected results</b>, which are
      commonly used for assignments that are more functional in nature,
      rather than focused on objects or whole programs. In some ways,
      stdin/stdout testing is a special case of flexible input/output tables
      (just with larger, string-only values).
    </p></li>
    <li><p><b>Native XUnit-style</b> executable software tests (such as JUnit,
      CxxTest, NUnit, pyunit, etc.). These would be language-specific, but
      are already used widely enough, and are expressive enough, that
      they play an important role.
    </p></li>
    <li><p>A <b>custom testing DSL</b> called
      <a href="../PEMLtest/">PEMLtest</a> designed to express a wide variety
      of software tests in a more concise, lighter weight way than XUnit
      programming. This DSL supports writing tests for multiple programming
      languages, and test specs can be translated down to executable
      XUnit-style tests in the target language.
    </p></li>
    <li><p><b>Generator-based strategies</b> where inputs of some form and
      expected outputs are programmatically generated fresh for every
      attempt.
    </p></li>
    <li><p><b>Reference solution strategies</b> where input comes from
      one of the approaches covered another way, but expected output is
      produced by running a reference solution provided by the
      exercise author, instead of being described as part of the tests.
    </p></li>
  </ul>
    
  <p>We believe that many such test expression formats can all be
    represented as a sequence (array) of hashes, and should naturally fit
    into a data model that is readily expressable in PEML, YAML, or JSON.
  </p>
  <p>We also expect that a variety of test case attributes (visibility,
    weight, scoring, etc.) may cleanly fit in as additional keys within
    a hash representing a test, and thus may be applicable to many of
    the formats listed above.
  </p>
  <p>
    OK, more to come later! ...
  </p>
</section>

<!--    
    influences:
    rspec
    cucumber
    xunit (starting with junit)
    xtest

<pre>
---
# top-level sequence of hashes
- describe: some string
  type: ...
  test: (same as describe)
  tests: (same as describe)
  test_for: (same as describe)
  tests_for: (same as describe)
  context: (same as describe)
  contexts: (same as describe)
  it: (same as describe)
  examples: (same as describe)
  examples_for: (same as describe)
  subject: string (or maybe a hash of name/initialization?)
  given:
    name:
    type:
    initialization:





  imports: string
  helpers: string (covers let, etc.)
  options:
    grading:
      weight:
      visibility: example, hidden
        show: input, output, expected output, input code, expected code
      category: tool-specific label (public, instructor, example, etc.)
    matching:
      string normalization options
  
  before:
  
  files: [{pathname: ..., content: ...}, {location: url}, ...]
  given: (same as subject?)
  given_input:
  when:  
  invariant:
  then:
   [ { clause: ..., parameters = { ... }, use reference: { subject: ..., reference: ...}}]
  then_output:
  then_output_matches:
  then_reference_output: (grade by reference implementation)
  
  after:

  
  suites: (nested as deeply as desired,)







  name:
  description:
  weight:
  type: (unit test, IO test, JavaX.Swing test, code quality test, custom)
  files: (injected)
  failure message:
  show unit code:
  show output:
  
  # for IO:
  automatically generate output:
  show input:
  show output:
  show correct output:
  show diff:
  enable output regex:
</pre>
-->
